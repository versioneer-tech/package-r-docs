{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Manage Data at Scale","text":""},{"location":"#overcoming-common-challenges","title":"Overcoming Common Challenges","text":"<p>In today\u2019s digital landscape, data is one of your most powerful assets. However, creating, managing, and distributing data products at scale presents constant challenges. From complex storage needs to coordinating data flow across various clouds, keeping a clear, organized view of your data journey can be overwhelming.</p>"},{"location":"#are-these-common-problems-holding-you-back","title":"Are These Common Problems Holding You Back?","text":""},{"location":"#1-massive-data-volumes-sitting-in-storage","title":"1. Massive Data Volumes Sitting in Storage","text":"<p>You've invested time and effort into generating valuable data products, yet they\u2019re often left sitting in storage environments. The challenge: how can you efficiently package and deliver relevant data to your customers without unnecessary steps, delays, or duplications on additional servers? Making stored data accessible, useful, and ready to deploy is critical to unlocking its full potential.</p>"},{"location":"#2-data-distributed-across-multiple-clouds","title":"2. Data Distributed Across Multiple Clouds","text":"<p>Today, data lives in many places\u2014across various clouds and data centers. To be effective, you need a clear view of where each data product resides and a seamless way to access and deliver it. However, every storage layer has its own tooling and processes, making it difficult to track, package, and distribute data efficiently. </p>"},{"location":"#3-security-and-access-management-across-environments","title":"3. Security and Access Management Across Environments","text":"<p>Managing secure access across multi-cloud environments is crucial, but it's a complex task. In some cases, you may want to distribute a simple, private URL, while in others, you need to grant explicit access to specific users or teams. Yet relying on each cloud provider\u2019s proprietary IAM (Identity and Access Management) tools limits flexibility and makes centralized access tracking difficult. How can you manage access effectively across varied environments without being restricted by individual cloud IAM systems?</p>"},{"location":"#4-evolving-data-products-to-stay-competitive","title":"4. Evolving Data Products to Stay Competitive","text":"<p>Data isn't static. As your data products evolve and improve, it\u2019s important to track which customers received which versions to provide timely support and targeted solutions. For your team, this also means ensuring they can efficiently experiment, improve, and stage data products without disrupting published versions. The challenge lies in making only the necessary data available without duplicating entire datasets.</p>"},{"location":"#introducing-package-r-your-solution-for-scalable-data-management","title":"Introducing Package R: Your Solution for Scalable Data Management","text":"<p>Package R is a comprehensive tool suite designed for teams tackling these exact challenges. It isn\u2019t a new platform and doesn\u2019t attempt to reinvent the wheel. Instead, it integrates seamlessly on top of commodity object storage and adheres to established industry practices to enhance collaboration on large datasets, ensure secure access, and streamline data distribution.</p> <p>With Package R, you can:</p> <ul> <li>Seamlessly access, organize, and package data across storage environments.</li> <li>Manage secure, centralized access across clouds using your IAM solutions of choice.</li> <li>Collaborate and evolve data products effectively, keeping them ready to deploy at every stage.</li> </ul> <p>Unlock the full potential of your data with Package R and transform how your team works with data products at scale.</p>"},{"location":"components/","title":"Components","text":"<p>The packageR UI is designed to streamline the creation of data packages and facilitate efficient data sharing. Connected object storage is displayed in a folder-like structure, allowing users to browse and reference items easily through copy-like operations. Data can also be seamlessly shared for dedicated audience via simple button click.</p> <p>The packageR API complements the UI by enabling API-only access, such as generating presigned URLs or automating the package curation process.</p>"},{"location":"components/api/","title":"API","text":""},{"location":"components/ui/","title":"UI","text":"<p>Fileexplorer like browsing and sharing are the main capabilities</p>"},{"location":"components/ui/#how-it-works","title":"How It Works","text":"<ol> <li>Access the UI:</li> <li> <p>Users can navigate to the <code>packageR</code> UI browse to manage their curated datasets. The interface provides options to view the data package structure and access metadata associated with the datasets.</p> </li> <li> <p>Generate Presigned URLs:</p> </li> <li>Instead of directly browsing and downloading content, users can opt to generate presigned URLs for the data files they wish to share. This allows others to access the data securely without exposing the actual storage locations.</li> <li> <p>Users can select the specific files or folders they want to share and initiate the URL generation process.</p> </li> <li> <p>Share for Offline Download:</p> </li> <li>The generated presigned URLs can be used for offline downloading of files. These URLs can be accessed via command-line tools like <code>curl</code> or <code>wget</code>, making it easy to retrieve data without needing to log into the UI.</li> <li>Each presigned URL is temporary and grants access only to the designated files, ensuring that data remains secure and controlled.</li> </ol>"},{"location":"components/ui/#benefits","title":"Benefits:","text":"<ul> <li>Secure Access Control: Presigned URLs provide a secure way to share data without exposing the underlying object storage details.</li> <li>Convenient Offline Access: Users can easily download datasets using familiar command-line tools, streamlining the workflow for data retrieval.</li> <li>Flexibility: The ability to generate URLs on-demand allows for tailored sharing, giving users control over what data is shared and with whom.</li> </ul> <p>This mechanism not only enhances data accessibility but also ensures that data governance principles are upheld, allowing for effective collaboration while safeguarding sensitive information.</p>"},{"location":"intro/components/","title":"Architecture","text":"<p>The architecture of <code>packageR</code> is organized into three key components, each designed to provide a seamless and secure data management experience.</p>"},{"location":"intro/components/#1-packager-ui","title":"1. <code>packageR</code> UI","text":"<p>The <code>packageR</code> UI features a file-browser interface, making it intuitive for users to manage and curate data packages. Users can navigate connected storage sources (e.g., object storage buckets), select individual data assets or folders (prefixes), and register them within packages. Once data is registered, it can be shared with selected audiences. The shared data can then be either downloaded directly or staged to a repository, allowing tools like <code>dvc</code> or <code>git annex</code> to provide robust version control and audit capabilities, ensuring reproducibility and data integrity.</p>"},{"location":"intro/components/#2-packager-api","title":"2. <code>packageR</code> API","text":"<p>The <code>packageR</code> API provides the necessary file-system abstraction functionality, including registering external data as pointer files and managing secure data access. When a data consumer requests access, the API verifies their permissions and then generates a presigned URL, either redirecting or providing direct access. This setup ensures that data remains securely accessible without requiring users to handle storage credentials. Additionally, the API supports connecting and browsing various storage sources, making it a central point for secure data operations.</p>"},{"location":"intro/components/#3-source-management","title":"3. Source Management","text":"<p><code>packageR</code> offers a unified view of all data assets across connected storage sources (e.g., object storage buckets) by utilizing file system mounts. Mounting methods are flexible, supporting widely-used options like FUSE with <code>s3fs</code>. For operation deployment scenarios using Kubernetes, the open-source tool sourceD can streamline the mounting process, which is also utilized by Versioneer in operational management. For each mounted source, <code>packageR</code> requires necessary credentials to generate presigned URLs. This can be configured with direct secret mounts or through integrated <code>sourceD</code> tools, ensuring secure and efficient data access.</p>"},{"location":"intro/features/","title":"Key Features","text":"<p><code>packageR</code> builds on established practices and integrates seamlessly with popular open-source tools, enabling data package management and sharing without specialized infrastructure. This flexibility enhances standard workflows with functionality that:</p> <ul> <li> <p>Unifies the creation of presigned URLs for diverse object storage platforms, such as AWS S3, MinIO, and Ceph RadosGW. It supports both private link URLs or explicit identity-based access grants through your chosen Identity Provider, delivering secure and adaptable access control.</p> </li> <li> <p>Simplifies data packaging by offering a unified view of all data assets, even when spread across multiple storage locations. Through symbolic link-like \u201cpointer\u201d files, <code>packageR</code> streamlines data dissemination and staging, automatically translating these pointers for client tools as needed. This allows for efficient bulk (fully resolved) or selective (streamed) data access.</p> </li> </ul>"},{"location":"intro/features/#how-packager-helps","title":"How <code>packageR</code> Helps","text":"<p><code>packageR</code> incorporates the following capabilities:</p>"},{"location":"intro/features/#1-streamlined-data-package-generation","title":"1. Streamlined Data Package Generation","text":"<p><code>packageR</code> allows you to easily connect storage sources (like object storage buckets) to gain a comprehensive overview of your data assets. By mimicking simple copy operations, <code>packageR</code> creates symbolic link-like \u201cpointer\u201d files to build organized data package hierarchies that can then be shared.</p> <p>Benefit: No actual data is duplicated; instead, it\u2019s a pure metadata operation using familiar file system operations.</p>"},{"location":"intro/features/#2-operates-without-proprietary-infrastructure","title":"2. Operates without Proprietary Infrastructure","text":"<p><code>packageR</code> works on top of standard commodity object storage, compatible with a wide array of OpenID Connect Identity Providers. This allows you to work where storage is most cost-effective, without the need to transfer data, and you retain control over data sharing.</p> <p>Benefit: Continue using established storage solutions like AWS S3 or MinIO and share data seamlessly with external providers like GitHub or internal users via your enterprise SSO system.</p>"},{"location":"intro/features/#3-simple-and-secure-data-sharing-via-git","title":"3. Simple and Secure Data Sharing via Git","text":"<p><code>packageR</code> supports sharing data packages through private link URLs or by selecting specific users or teams within your connected security realm, following familiar Git-based workflows. Shared folders can contain static data packages with any number of assets or dynamic, live sources, with consumption managed through presigned URLs.</p> <p>Benefit: Apply granular permissions to share exactly what you want, without complex IAM policies or compromising security.</p>"},{"location":"intro/features/#4-transparent-data-resolution-for-pointer-files","title":"4. Transparent Data Resolution for Pointer Files","text":"<p><code>packageR</code> enables clients to download pointer files for bulk resolution with tools like <code>wget</code> or <code>curl</code> and can stage data into repositories through integrations with <code>dvc</code> and <code>git annex</code>. Data is securely accessed via presigned URLs for authorized users, meaning repositories with pointer files can be shared without exposing underlying storage credentials.</p> <p>Benefit: Automatically resolve pointer files securely using preferred client tools without compromising data security.</p>"},{"location":"intro/features/#how-packager-compares-to-other-data-management-tools","title":"How <code>packageR</code> Compares to Other Data Management Tools","text":"<p><code>packageR</code> doesn\u2019t replace existing tools but instead complements them for enhanced data collaboration. By integrating with <code>git annex</code>, <code>packageR</code> streamlines the data-sharing process and can be extended to work with <code>dvc</code>. This approach maintains the scalability of external storage solutions, unlike <code>git lfs</code>, which may face challenges with large file volumes, making <code>git annex</code> a more scalable choice.</p>"},{"location":"intro/features/#only-one-piece-of-the-larger-data-management-puzzle","title":"Only One Piece of the Larger Data Management Puzzle","text":"<p><code>packageR</code> focuses on data sharing and distribution, while for end-to-end data product creation, we recommend exploring other solutions from Versioneer, which integrate seamlessly with commodity storage and provide:</p> <ul> <li>Stable snapshots to preserve data states for consistent access.</li> <li>Time-travel capabilities for navigating data versions.</li> <li>Data deduplication to optimize storage by removing redundancies.</li> <li>Automated data expiration to efficiently manage data lifecycles.</li> </ul> <p>These capabilities integrate well with <code>git</code> workflows extended by <code>git annex</code> or <code>dvc</code>, enhancing your overall data management strategy.</p>"},{"location":"intro/features/#the-data-management-shared-responsibility-model","title":"The Data Management Shared Responsibility Model","text":"<p>At <code>packageR</code>, we advocate for solid engineering practices as the foundation of successful data management. Adopting effective engineering methodologies allows operators to introduce the right tools to support large-scale, high-volume data management\u2014a shared responsibility for achieving collaborative success and improved outcomes.</p> <p>By embracing these principles, teams can leverage <code>packageR</code> alongside other tools to enhance their data management capabilities, driving greater collaborative success and better results.</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/e2e/","title":"End-to-End Scenario: Curating and Sharing a Dataset","text":""},{"location":"tutorials/e2e/#1-data-curator","title":"1. Data Curator","text":""},{"location":"tutorials/e2e/#objective","title":"Objective:","text":"<p>The Data Curator aims to assemble and organize a dataset by referencing external files and creating a structured data package.</p>"},{"location":"tutorials/e2e/#steps","title":"Steps:","text":"<ul> <li>Access the <code>packageR</code> UI:</li> <li> <p>The Data Curator logs into the <code>packageR</code> UI and navigates to the section for curating datasets.</p> </li> <li> <p>Reference External Data:</p> </li> <li>The curator selects the option to register external data. </li> <li>They enter URLs pointing to data objects stored in cloud storage (e.g., Amazon S3, Google Cloud Storage) using the <code>git annex addurl &lt;url&gt;</code> command in the UI.</li> <li> <p>As they add URLs, they organize them into a meaningful folder hierarchy, making the dataset easy to navigate.</p> </li> <li> <p>Add Documentation:</p> </li> <li> <p>The curator creates a <code>README.md</code> file directly within the UI to provide context for the dataset. This file includes details on the data's origin, structure, and any relevant metadata.</p> </li> <li> <p>Sync with Git-Repository:</p> </li> <li>Once the data package is ready, the curator pushes the changes to a Git-Repository (e.g., GitHub or GitLab) using the UI's built-in sync functionality. </li> <li>Behind the scenes, this corresponds to executing a standard <code>git push</code> command:     <pre><code>git push origin main\n</code></pre></li> </ul>"},{"location":"tutorials/e2e/#2-data-governance","title":"2. Data Governance","text":""},{"location":"tutorials/e2e/#objective_1","title":"Objective:","text":"<p>The Data Governance professional ensures that access to the data package is properly managed, maintaining security and compliance.</p>"},{"location":"tutorials/e2e/#steps_1","title":"Steps:","text":"<ul> <li>Set Repository Visibility:</li> <li>The Data Governance professional accesses the Git-Repository settings via GitHub/GitLab.</li> <li> <p>They decide to make the repository private, ensuring that only authorized users can access the actual data while metadata remains visible.</p> </li> <li> <p>Manage Permissions:</p> </li> <li>They add users who require access to the dataset, assigning at least read permissions. This enables those users to resolve data references using <code>git annex</code>.</li> <li> <p>For example, if a Data Scientist needs access, the governance professional would add them to the repository with the appropriate role.</p> </li> <li> <p>Monitor Access Control:</p> </li> <li>The Data Governance professional periodically reviews permissions and ensures compliance with data governance policies, adjusting access as needed.</li> </ul>"},{"location":"tutorials/e2e/#3-data-scientistcollaborator","title":"3. Data Scientist/Collaborator","text":""},{"location":"tutorials/e2e/#objective_2","title":"Objective:","text":"<p>The Data Scientist or Collaborator works with the curated dataset, selectively downloading the necessary data for analysis and potentially contributing back to the repository.</p>"},{"location":"tutorials/e2e/#steps_2","title":"Steps:","text":"<ul> <li>Clone the Repository:</li> <li>The Data Scientist clones the Git-Repository to their local machine using the command:     <pre><code>git clone https://github.com/username/repo.git\n</code></pre></li> <li> <p>This action retrieves the folder hierarchy and metadata (including the <code>README.md</code>).</p> </li> <li> <p>Resolve Data:</p> </li> <li>After reviewing the contents, the Data Scientist identifies specific files they need for their analysis.</li> <li>They run the command to download a specific file or folder using <code>git annex</code>:     <pre><code>`git annex` get &lt;file_or_folder_name&gt;\n</code></pre></li> <li> <p>This retrieves the required data files without needing to download the entire dataset.</p> </li> <li> <p>Add Contributions (if permissions allow):</p> </li> <li>If the Data Scientist has write permissions, they can add new data references or documentation.</li> <li>For example, they might create a new analysis report and add it to the repository:     <pre><code>echo \"Analysis report\" &gt; analysis_report.md\ngit add analysis_report.md\ngit commit -m \"Added analysis report\"\ngit push origin main\n</code></pre></li> </ul>"},{"location":"tutorials/e2e/#summary","title":"Summary","text":"<p>Through this end-to-end scenario, each persona interacts with <code>packageR</code> at different stages of the data lifecycle:</p> <ul> <li>The Data Curator focuses on organizing and structuring datasets by referencing external data and adding necessary documentation.</li> <li>The Data Governance professional manages repository visibility and permissions, ensuring secure access to sensitive data.</li> <li>The Data Scientist utilizes the curated data for analysis, downloading only the required files and contributing back to the repository when necessary.</li> </ul> <p>This collaborative workflow illustrates the power of <code>packageR</code> in enabling efficient data management, sharing, and analysis across different user roles while maintaining security and flexibility.</p>"},{"location":"workflows/","title":"Workflows","text":"<p><code>packageR</code> supports various personas, each with specific workflows designed to fit their roles and responsibilities.</p>"},{"location":"workflows/#1-data-curator","title":"1. Data Curator","text":"<p>As a Data Curator, your responsibility is to manage and organize datasets by referencing external files stored in cloud platforms. You create a well-structured hierarchy of data assets and can include documentation such as a <code>README.md</code>. Once your data is organized, you share these data packages to ensure easy access without the need for proprietary infrastructure.</p>"},{"location":"workflows/#key-responsibilities","title":"Key Responsibilities:","text":"<ul> <li>Organize and reference data stored in cloud platforms.</li> <li>Add metadata and documentation to the data packages.</li> <li>Share data packages with relevant stakeholders.</li> </ul>"},{"location":"workflows/#2-data-governance","title":"2. Data Governance","text":"<p>As part of the Data Governance team, you oversee access management for both cloud storage and the data packages shared with users. You ensure that appropriate permissions are granted and maintained to manage secure and compliant data access.</p>"},{"location":"workflows/#key-responsibilities_1","title":"Key Responsibilities:","text":"<ul> <li>Manage connections to cloud storage.</li> <li>Set or oversee access controls for shared data packages.</li> </ul>"},{"location":"workflows/#3a-customer","title":"3a. Customer","text":"<p>As a Customer, you download data for initial inspection and then integrate it into your operations. You begin by selectively downloading the data you need, and later leverage bulk download capabilities to acquire all necessary assets. You interact with curated datasets stored in repositories for more efficient consumption.</p>"},{"location":"workflows/#key-responsibilities_2","title":"Key Responsibilities:","text":"<ul> <li>Download and inspect data.</li> <li>Integrate the data into your operational environment.</li> </ul>"},{"location":"workflows/#3b-data-scientist-collaborator","title":"3b. Data Scientist &amp; Collaborator","text":"<p>As a Data Scientist or Collaborator, you work with curated datasets in repositories, utilizing tools like <code>dvc</code> or <code>git annex</code> to selectively download the necessary data. With proper permissions, you can also contribute new data, files, or documentation to the repository, collaborating seamlessly within GitHub or GitLab.</p>"},{"location":"workflows/#key-responsibilities_3","title":"Key Responsibilities:","text":"<ul> <li>Stage shared assets in Git repositories.</li> <li>Contribute new data, files, or documentation to the repository.</li> <li>Collaborate within GitHub/GitLab using familiar tools and workflows.</li> </ul>"},{"location":"workflows/data-curation/","title":"Data Curator","text":"<p>As a Data Curator, you play a vital role in assembling and organizing datasets. With <code>packageR</code>, you can efficiently curate data by referencing external files (objects stored in cloud storage like S3, GCS, etc.) and arranging them into meaningful folder hierarchies within a Git-Repository. Here\u2019s a step-by-step guide on how to leverage <code>packageR</code> for your curation tasks:</p>"},{"location":"workflows/data-curation/#tasks-and-responsibilities","title":"Tasks and Responsibilities:","text":"<ul> <li>Curate Data: Organize data from multiple external sources, including various cloud storage buckets or URLs.</li> <li>Reference External Data: Register external data files (URLs pointing to objects in different clouds) without duplicating the files themselves.</li> <li>Add Additional Documentation: Include metadata or supporting files, such as <code>README.md</code>, to describe the dataset and its usage.</li> <li>Share Data Packages: Share your curated folder structure, metadata, and URLs as data package.</li> </ul>"},{"location":"workflows/data-governance/","title":"Data Governance","text":"<p>As a Data Governance professional, your main role is to manage and regulate access to both cloud storage and data package repositories. You decide if access should be public or private, ensuring the necessary permissions are in place for secure collaborator access. Furthermore, you oversee which object storage solutions are connected to the <code>packageR</code> UI for data package curation.</p>"},{"location":"workflows/data-governance/#tasks-and-responsibilities","title":"Tasks and Responsibilities:","text":"<ul> <li>Manage Connected Object Storage: Select which cloud object storage services integrate with the <code>packageR</code> UI for data curation.</li> <li>Manage  Access: Control access grant by managing permissions.</li> </ul>"}]}